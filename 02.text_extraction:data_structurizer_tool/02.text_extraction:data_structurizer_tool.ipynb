{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement\n",
        "To develop data structurizer tool which is capable of extracting specific, relevant information from a variety of unstructured documents, including invoices, receipts, bills, medical prescriptions, and general documents.\n",
        "\n",
        "The extracted data should be organized into a structured DataFrame format, providing a clean and usable dataset for further analysis or applications."
      ],
      "metadata": {
        "id": "3QMIulUL1KIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expected Output\n",
        "\n",
        "1. Working Notebook in any of the platform using python\n",
        "2. The final output should be a well-structured DataFrame containing the extracted data.\n",
        "3. Each DataFrame should represent a single document, and each column should correspond to a specific data element (e.g., invoice number, date, total amount, item descriptions)."
      ],
      "metadata": {
        "id": "M-PzMOEH1zrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Considerations\n",
        "1. Document Format: Consider the variety of document formats you'll be dealing with (e.g., PDF, Word, images) and choose appropriate tools or techniques for each.\n",
        "2. Prompt Engineering: Craft effective prompts to guide the LLM in extracting the desired information.\n",
        "3. Model Selection: Choose an LLM that is suitable for the task and the available resources.\n"
      ],
      "metadata": {
        "id": "b1ZxFmMD2aax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Criteria\n",
        "1. Data Quality and Accuracy:\n",
        "> * Completeness: Were all relevant data elements extracted from the documents?\n",
        "> * Accuracy: Is the extracted data correct and consistent with the original documents?\n",
        "> * Consistency: Are there any inconsistencies or contradictions within the extracted data?\n",
        "\n",
        "2. Data Structurization:\n",
        "> * Data Organization: Is the data organized in a clear and understandable manner within the DataFrame?\n",
        "> * Data Quality: Are there any missing values, inconsistencies, or errors in the structured data?\n",
        "\n",
        "3. LLM Usage:\n",
        "> * Prompt Engineering: Were the prompts used to guide the LLM effective in extracting the desired information?\n",
        "> * Model Selection: Was the chosen LLM appropriate for the task?\n",
        "> * Fine-Tuning: Was the LLM effectively fine-tuned on a relevant dataset?\n",
        "\n",
        "4. Code Quality:\n",
        "> * Readability: Is the code well-structured, commented, and easy to understand?\n",
        "> * Efficiency: Is the code efficient in terms of computational resources and execution time?"
      ],
      "metadata": {
        "id": "JHZwzykt2sCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solution Overview"
      ],
      "metadata": {
        "id": "fK4B_S503Zkw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Req. Libraries"
      ],
      "metadata": {
        "id": "CGWbMLH23r-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -q tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPMyUv1qX5A1",
        "outputId": "35e80755-fe1a-4381-9e36-d0f0af8c3ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -q tesseract-ocr-eng"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvCtUNhmYFy-",
        "outputId": "5edad79a-1e4c-40b0-8607-41dadb0e6305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "tesseract-ocr-eng is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain-community PyMuPDF unstructured[all-docs] pytesseract nltk langchain-openai docx2txt"
      ],
      "metadata": {
        "id": "q2sPojZXTb4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Req. Libraries"
      ],
      "metadata": {
        "id": "jebsWRWX3ypp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTING REQ MODULES\n",
        "import os\n",
        "import fitz\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain_community.document_loaders.image import UnstructuredImageLoader\n",
        "from langchain_community.document_loaders import Docx2txtLoader\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0obwqE-To1A",
        "outputId": "34fad737-93bf-4e1e-cd1b-991a9b7c802f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"chatgpt-4o-latest\" #\"gpt-4o-mini-2024-07-18\"\n",
        "openai_api_key = \"\" # add openai key\n",
        "input_folder_path = '/content/input/'\n",
        "csv_path = '/content/csv/'"
      ],
      "metadata": {
        "id": "lAPmjfgR5Vn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt"
      ],
      "metadata": {
        "id": "AlRH1n2eD1gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PROMPT TEMPLATE - INPUT PARAMETER: data\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "**Context:**\n",
        "You are tasked with extracting important information from a given text and creating a JSON file where each column corresponds to a specific data element along with metadata.\n",
        "\n",
        "**Roles:**\n",
        "Assume the role of a data extraction specialist who is adept at identifying key pieces of information and organizing them systematically.\n",
        "\n",
        "**Instructions:**\n",
        "1. Read the given text thoroughly to understand the context and the key pieces of information.\n",
        "2. Identify and extract the all information that should be included in the JSON file.\n",
        "3. Organize the extracted information into specific data elements, ensuring that each element has a corresponding key.\n",
        "4. Include metadata that captures additional contextual information or attributes relevant to the extracted data.\n",
        "5. Format the extracted information and metadata into a JSON structure such as {{\\'key1\\':\\'value1\\',\\'key2\\':\\'value2\\',.........,\\'metadata\\':\\'actual document information\\'}}.\n",
        "6. Ensure that the JSON file is well-structured and follows the specified format.\n",
        "\n",
        "**Constraints:**\n",
        "- Ensure all extracted data is accurate and relevant.\n",
        "- Follow the JSON format strictly without any deviations.\n",
        "- Include all necessary metadata to provide context for the extracted information.\n",
        "\n",
        "**Examples:**\n",
        "- Given text: \"John Doe, a software engineer, joined XYZ Corp in 2020. His email is john.doe@xyz.com.\"\n",
        "  - Extracted JSON: {{\\'name\\':\\'John Doe\\',\\'occupation\\':\\'software engineer\\',\\'company\\':\\'XYZ Corp\\',\\'year_of_joining\\':\\'2020\\',\\'email\\':\\'john.doe@xyz.com\\',\\'metadata\\':\\'Extracted from a personnel record\\'}}\n",
        "\n",
        "**Output Format:**\n",
        "- The output should be a JSON file formatted as follows:\n",
        "  ```\n",
        "  {{\n",
        "    \\'key1\\':\\'value1\\',\n",
        "    \\'key2\\':\\'value2\\',\n",
        "    ...\n",
        "    \\'metadata\\':\\'actual document information\\'\n",
        "  }}\n",
        "  ```\n",
        "\n",
        "**Evaluation Criteria:**\n",
        "- Completeness: All key pieces of information are extracted and included in the JSON file.\n",
        "- Accuracy: Extracted data should be accurate and free from errors.\n",
        "- Structure: The JSON file should be well-organized and follow the specified format.\n",
        "- Relevance: Only the important and relevant information should be included.\n",
        "\n",
        "**Actual_problem:**\n",
        "{data}\"\"\""
      ],
      "metadata": {
        "id": "pt73iJzc4siw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATION OF LANGCHAIN SIMPLE CHAIN\n",
        "model = ChatOpenAI(model=model_name,temperature=0,openai_api_key=openai_api_key)\n",
        "parser = JsonOutputParser()\n",
        "prompt = PromptTemplate(\n",
        "    template=PROMPT_TEMPLATE, input_variables=[\"data\"],partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "chain = prompt | model | parser"
      ],
      "metadata": {
        "id": "PkvqmE_r4nHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Extraction"
      ],
      "metadata": {
        "id": "wuD3yFpBD9pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary to map file extensions to loaders\n",
        "LOADERS = {\n",
        "    ('.pdf', '.PDF'): PyMuPDFLoader,\n",
        "    ('.png', '.jpg', '.jpeg'): UnstructuredImageLoader,\n",
        "    ('.doc', '.docx'): Docx2txtLoader\n",
        "}\n",
        "\n",
        "folder_path = input_folder_path\n",
        "\n",
        "# Iterate over files in the directory\n",
        "for filename in os.listdir(folder_path):\n",
        "    file_extension = os.path.splitext(filename)[1].lower()\n",
        "    loader_class = None\n",
        "\n",
        "    # Determine the appropriate loader\n",
        "    for ext_tuple, loader in LOADERS.items():\n",
        "        if file_extension in ext_tuple:\n",
        "            loader_class = loader\n",
        "            break\n",
        "\n",
        "    if loader_class is None:\n",
        "        # Handle files without a known loader\n",
        "        loader_class = UnstructuredImageLoader\n",
        "\n",
        "    try:\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        loader = loader_class(file_path)\n",
        "        docs = loader.load()\n",
        "\n",
        "        json_data = chain.invoke({\"data\": docs})\n",
        "        df = pd.json_normalize(json_data)\n",
        "        original_filename = docs[0].metadata['source'].split('/')[-1]\n",
        "        base_filename = os.path.splitext(original_filename)[0]\n",
        "        df.to_csv(f'{csv_path}{base_filename}.csv', index=False)\n",
        "\n",
        "        print(f\"Data is extracted from {original_filename} and written to {base_filename}.csv\")\n",
        "        print(\"....\"*30)\n",
        "        display(df)\n",
        "        print(\" \")\n",
        "        print(\"====\"*30)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred with file {filename}: {e}\")\n",
        "        continue"
      ],
      "metadata": {
        "id": "XTxEzQUnTPYp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
