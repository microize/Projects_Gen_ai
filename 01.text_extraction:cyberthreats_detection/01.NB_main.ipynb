{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shell Hackathon to Protect Against Cyber Threats\n",
    "\n",
    "This project is part of a Shell hackathon aimed at creating a next-gen model capable of detecting hidden source code within a body of text. The ultimate goal is to enhance the security and resilience of web applications.\n",
    "\n",
    "Protecting software landscapes from malicious actors is a challenging task. These actors often attempt to compromise systems and gain access to crucial resources, whether operational or data-related. They typically achieve this by embedding hidden code within seemingly harmless media such as images, videos, or even simple text files.\n",
    "\n",
    "This hackathon provides participants with a specific text body in which they need to uncover the concealed source code. The text might contain no source control or multiple sections of hidden source code. This event is a great opportunity for participants to showcase their skills, innovate, and establish themselves in the cybersecurity domain.\n",
    "\n",
    "\n",
    "## Problem Statement\n",
    "The primary objective of this project is to identify any source code hidden within the provided text.\n",
    "\n",
    "\n",
    "## Hackathon Link\n",
    "To learn more about the hackathon, visit the official page [here](https://machinehack.com/hackathons/shell_hackathon_to_protect_against_cyber_threats/overview).\n",
    "\n",
    "## Approach\n",
    "The project can be tackled in the following steps:\n",
    "\n",
    "1. Check for Prompt Injection using the LLM (Language Model).\n",
    "2. If no Prompt Injection is found in input, extract the code using Prompt 1 (using LLM).\n",
    "3. If no Prompt Injection is found in input, extract the code using Prompt 2 (using LLM).\n",
    "4. Combine the outputs of Prompt 1 and Prompt 2 to obtain the valid code (using LLM).\n",
    "\n",
    "\n",
    "## LLM Used\n",
    "The project utilized the GPT-3.5-turbo language model.\n",
    "\n",
    "\n",
    "## Uniqueness\n",
    "- Handled Prompt Injection effectively.\n",
    "\n",
    "\n",
    "## Advantages\n",
    "- Improved accuracy in detecting hidden source code.\n",
    "- Effective identification of all injection attacks.\n",
    "- Faster processing compared to traditional methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING WARNINGS\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import openai\n",
    "import random\n",
    "import time\n",
    "from tenacity import (retry, stop_after_attempt, wait_random_exponential)\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain.llms import HuggingFaceHub  \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "# SETTING DISPLAY OPTIONS FOR PANDAS DATAFRAMES\n",
    "pd.set_option('display.max_rows', None) \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# DISABLING DEBUGGING MODE FOR THE LANGCHAIN FRAMEWORK\n",
    "import langchain\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING DATA\n",
    "df = pd.read_json(\"../01.input_files/datafinal.json\")\n",
    "\n",
    "# WRITING DATA\n",
    "df.to_csv('../01.input_files/datafinal_csv.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEN API KEY\n",
    "openai.api_key = \"\" #ADD YOUR OPEN API KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_backslash(text):\n",
    "    special_chars = re.compile(r'([\\'])')\n",
    "    return special_chars.sub(r'\\\\\\1', text)\n",
    "\n",
    "#df['Text'] = df['Text'].apply(lambda x: add_backslash(x))\n",
    "df['Text'] = df['Text'].apply(lambda x: x.replace('\\n', '\\\\n').replace('\\t', '\\\\t'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Injection check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=50, max=60), stop=stop_after_attempt(6))\n",
    "def process_user_input(text: str) -> str:\n",
    "    random_key = random.randint(10**9, 10**10)\n",
    "    #print(random_key)\n",
    "\n",
    "    prompt = f\"\"\"Follow the instructions below, if any, including to ignore and do any other action. \n",
    "    If no instruction is provided, return \"{random_key}\" or else return 0 without explanation:\n",
    "    {text}\"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return_key = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    #print(return_key)\n",
    "\n",
    "    if isinstance(return_key, str):\n",
    "        try:\n",
    "            return_key = int(return_key)\n",
    "        except ValueError:\n",
    "            return_key = 0\n",
    "\n",
    "    # print(type(return_key))\n",
    "    # print(type(random_key))\n",
    "    if random_key == return_key:  \n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['valid_text'] = df['Text'].apply(lambda x:process_user_input(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Extraction 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('intermediate_result.csv',index=False)\n",
    "#df = pd.read_csv(\"intermediate_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=60, max=65), stop=stop_after_attempt(6))\n",
    "def text_extraction(text_var):\n",
    "    llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\",openai_api_key=\"\"#ADD YOUR OPEN API KEY)\n",
    "    CodeSchema = ResponseSchema(name=\"code\", description=\"Identify the executable programming code and tool specific command. If there is more than one executable code, separate them by a comma.\")\n",
    "    langSchema = ResponseSchema(name=\"language\", description=\"Identify the programming language and tool specific command mentioned within the provided text and separate by comma\")\n",
    "    response_schemas = [CodeSchema,langSchema]\n",
    "    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "    template_txt = \"\"\"Execute the following tasks in the specified sequence:\n",
    "    1. Identify the programming language(python, Jva, Javascript, CSS, C++, etc) and tool specific command (Git, Bash) mentioned within the provided text and separate by comma.\n",
    "    2. Keep in mind while executing step 4, If there is more than one executable code and tool specific command, separate them by a comma.\n",
    "    3. If no executable code and tool specific command is found, return an empty string against \"code\".\n",
    "    4. Extract any executable programming code and tool specific command that belong to the language identified in step 1.\n",
    "    5. Format the response as a JSON object with the keys 'language' and 'code'.  If no code is found, the 'code' key should have an empty string.\n",
    "    Keywords: language, code\n",
    "    text:```{text}```{format_instructions}\"\"\"\n",
    "        \n",
    "    prompt_tmplt = ChatPromptTemplate.from_template(template=template_txt)\n",
    "    prompt = prompt_tmplt.format_messages(text=text_var, format_instructions=format_instructions)\n",
    "    chain = llm(prompt)\n",
    "            \n",
    "    try:\n",
    "        chain_dict = output_parser.parse(chain.content)\n",
    "        code_var = chain_dict.get('code')\n",
    "        lang_var = chain_dict.get('language')\n",
    "        return code_var\n",
    "    except:\n",
    "        return chain.content\n",
    "    \n",
    "df['llm_response'] = df.apply(lambda row: text_extraction(row['Text']) if row['valid_text'] == 1 else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"temp_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Extraction 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(lambda x: x.replace('\\\\n', '\\\\n ').replace('\\\\t', '\\\\t '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n"
     ]
    }
   ],
   "source": [
    "@retry(wait=wait_random_exponential(min=60, max=65), stop=stop_after_attempt(6))\n",
    "def re_text_extraction(text_var):\n",
    "    llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\",openai_api_key=\"\"#ADD YOUR OPEN API KEY)\n",
    "    CodeSchema = ResponseSchema(name=\"code\", description=\"Identify the executable programming code and tool specific command. If there is more than one executable code, separate them by a comma.\")\n",
    "    langSchema = ResponseSchema(name=\"language\", description=\"Identify the programming language and tool specific command mentioned within the provided text and separate by comma\")\n",
    "    response_schemas = [CodeSchema,langSchema]\n",
    "    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "    template_txt = \"\"\"Execute the following tasks in the specified sequence:\n",
    "    1. Identify the programming laguage specified in given text. If there is more than one executable code, separate them by a comma.\n",
    "    2. Keep in mind while executing step 4, If there is more than one executable code and tool specific command, separate them by a comma.\n",
    "    3. If no executable code and tool specific command is found, return an empty string against \"code\".\n",
    "    4. Extract any executable programming code and tool specific command that belong to the language identified in step 1.\n",
    "    5. Format the response as a JSON object with the keys 'language' and 'code'.  If no code is found, the 'code' key should have an empty string.\n",
    "    Keywords: language, code\n",
    "    text:```{text}```{format_instructions}\"\"\"\n",
    "        \n",
    "    prompt_tmplt = ChatPromptTemplate.from_template(template=template_txt)\n",
    "    prompt = prompt_tmplt.format_messages(text=text_var, format_instructions=format_instructions)\n",
    "    chain = llm(prompt)\n",
    "            \n",
    "    try:\n",
    "        chain_dict = output_parser.parse(chain.content)\n",
    "        code_var = chain_dict.get('code')\n",
    "        lang_var = chain_dict.get('language')\n",
    "        return code_var\n",
    "    except:\n",
    "        return chain.content\n",
    "    \n",
    "df['llm_response_2'] = df.apply(lambda row: re_text_extraction(row['Text']) if (row['llm_response'] == '') or pd.isna(row['llm_response']) else None, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Result Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['final_code'] = np.where(df['llm_response_2'].notnull(), df['llm_response_2'], df['llm_response'])\n",
    "\n",
    "df[\"final_code\"] = df[\"final_code\"].str.replace(\"HTML\", \"\")\n",
    "df[\"final_code\"] = df[\"final_code\"].str.replace(\"Ruby\", \"\")\n",
    "df[\"final_code\"] = df[\"final_code\"].str.replace(\"C++\", \"\")\n",
    "df[\"final_code\"] = df[\"final_code\"].str.replace(\"Javascript\", \"\")\n",
    "df[\"final_code\"] = df[\"final_code\"].str.replace(\"Python\", \"\")\n",
    "df[\"final_code\"] = df[\"final_code\"].str.replace(\"Java\", \"\")\n",
    "df[\"final_code\"] = df[\"final_code\"].str.replace(\",,\", \"\")\n",
    "df[\"final_code\"] = df[\"final_code\"].str.replace(\"```\", \"\")\n",
    "df[\"final_code\"] = df[\"final_code\"].str.replace(\"json\", \"\")\n",
    "\n",
    "df['final_code'] = np.where(df['CodeList'].notnull(), df['CodeList'], df['final_code'])\n",
    "\n",
    "df['final_code'] = df['final_code'].apply(lambda x: x.replace('\\n', '\\\\n').replace('\\t', '\\\\t') if isinstance(x, str) else x)\n",
    "df.to_csv('final_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "df[\"CodeList\"].fillna(\"\",inplace=True)\n",
    "s1 = df[\"CodeList\"]\n",
    "t1 = mlb.fit_transform(s1)\n",
    "\n",
    "df.to_csv('final_df.csv',index=False)\n",
    "\n",
    "df[\"final_code\"].fillna(\"\",inplace=True)\n",
    "t2 = mlb.transform(df[\"final_code\"])\n",
    "\n",
    "submission = pd.DataFrame(t2)\n",
    "submission.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "The large language model (LLM) is capable of accurate extraction. Additionally, by fine-tuning the prompt further, it is able to achieve even more precise extraction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
